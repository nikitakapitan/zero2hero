{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In this Colab we:\n",
    "1. import Shakespear romans (docs/shakespear.txt)\n",
    "2. prepare the data:\n",
    "    - vanilla tokenizer char -> index | ex. a -> 1\n",
    "    - prepare batch loader with block_size = 8 | ex. [15, 32, 7 .. 19] -> [13]\n",
    "3. create Neuro-based BigramLM\n",
    "    - forward ~ nn.Embedding(idx)\n",
    "    - generate next token\n",
    "4. train BLM\n",
    "\n",
    "   \n",
    "\n",
    "---------------------\n",
    "### Result 1 : sampling from Bi-Gram ~= sampling from Neuro approach.\n",
    "\n",
    "Neuro approach : train only character embeddings matrix W : output = SoftMax(OHE @ W)\n",
    "\n",
    "**Two Forward Pass Sampling are the same! (probabilities are literally the same)**\n",
    "\n",
    "Just like we manually count Bi-gram, we update the word embedding matrix so its WEIGHTS \"account\" the statistical bi-occurance of characters.\n",
    "\n",
    "----------------------\n",
    "\n",
    "### Result 2 : lower bounds for likelihood\n",
    "Upper bound for Likelihood (randomly uniformed guess)\n",
    "- 3.2958\n",
    "\n",
    "Lower bounds for likelohood based on k previous chars\n",
    "\n",
    "- k = 1 : 2.454 \n",
    "- k = 2 : 2.092 \n",
    "- k = 3 : 1.963\n",
    "----------------------\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import Shakespear text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved. \n",
      "\n",
      "len(text)=1115390\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "with open('../docs/shakespear.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(text[:173], f'\\n\\n{len(text)=}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Pre-process Shakespear\n",
    "## 2.1 Vanilla Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: hii there\n",
      "Encoding: [44, 45, 45, 1, 56, 44, 41, 54, 41]\n",
      "Decoding: hii there\n",
      "\n",
      "block_size=8\t train block:tensor([16, 45, 54, 55, 56,  1, 13, 45, 56])\n",
      "tensor([16]) -> 45\n",
      "tensor([16, 45]) -> 54\n",
      "tensor([16, 45, 54]) -> 55\n",
      "tensor([16, 45, 54, 55]) -> 56\n",
      "tensor([16, 45, 54, 55, 56]) -> 1\n",
      "tensor([16, 45, 54, 55, 56,  1]) -> 13\n",
      "tensor([16, 45, 54, 55, 56,  1, 13]) -> 45\n",
      "tensor([16, 45, 54, 55, 56,  1, 13, 45]) -> 56\n"
     ]
    }
   ],
   "source": [
    "chars, counts = np.unique(list(text), return_counts=True)\n",
    "counts = dict(zip(chars, counts))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "stoi = {c:i for i,c in enumerate(chars)}\n",
    "itos = {i:c for i,c in enumerate(chars)}\n",
    "\n",
    "encode = lambda s : [stoi[c] for c in s]\n",
    "decode = lambda l : ''.join([itos[i] for i in l])\n",
    "\n",
    "print('Text: hii there')\n",
    "print('Encoding:', encode('hii there'))\n",
    "print('Decoding:', decode(encode('hii there')))\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "\n",
    "split_ration = int(0.9 * len(data))\n",
    "train_data = data[:split_ration]\n",
    "val_data = data[split_ration:]\n",
    "\n",
    "block_size = 8\n",
    "print(f'\\n{block_size=}\\t train block:{train_data[:block_size+1]}')\n",
    "\n",
    "\n",
    "for t in range(block_size):\n",
    "    context = train_data[:t+1]\n",
    "    target = train_data[t+1]\n",
    "    print(f\"{context} -> {target}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Batch Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch of 4: \n",
      "\txb.shape=torch.Size([4, 8])\n",
      "\txb=tensor([[ 1, 37, 50, 40,  1, 49, 51, 54],\n",
      "        [37, 39, 47,  1, 37, 42, 56, 41],\n",
      "        [ 1, 45, 50,  1, 44, 41, 48, 48],\n",
      "        [44,  1, 38, 51, 61,  4,  1, 56]])\n",
      "\tyb.shape=torch.Size([4, 8])\n",
      "\tyb=tensor([[37, 50, 40,  1, 49, 51, 54, 41],\n",
      "        [39, 47,  1, 37, 42, 56, 41, 54],\n",
      "        [45, 50,  1, 44, 41, 48, 48,  9],\n",
      "        [ 1, 38, 51, 61,  4,  1, 56, 44]])\n",
      "Each batch will produce 8 examples:\n",
      "\ttensor([1])->37\n",
      "\ttensor([ 1, 37])->50\n",
      "\ttensor([ 1, 37, 50])->40\n",
      "\ttensor([ 1, 37, 50, 40])->1\n",
      "\ttensor([ 1, 37, 50, 40,  1])->49\n",
      "\ttensor([ 1, 37, 50, 40,  1, 49])->51\n",
      "\ttensor([ 1, 37, 50, 40,  1, 49, 51])->54\n",
      "\ttensor([ 1, 37, 50, 40,  1, 49, 51, 54])->41\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4\n",
    "def get_batch(data, batch_size):\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x,y\n",
    "\n",
    "xb,yb = get_batch(train_data, batch_size)\n",
    "print(f'Batch of {batch_size}: \\n\\t{xb.shape=}\\n\\t{xb=}\\n\\t{yb.shape=}\\n\\t{yb=}')\n",
    "\n",
    "print(f'Each batch will produce {block_size} examples:')\n",
    "for t in range(block_size):\n",
    "    context = xb[0, :t+1]\n",
    "    target = yb[0, t]\n",
    "    print(f'\\t{context}->{target}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Create Neuro-BigramLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out.shape=torch.Size([32, 63])\n",
      "CE_loss=4.35011100769043\n",
      "yman, I AwdzD,?zGbeBBBBKonkcWrctIq R-aqniJdHkr LkKU?scpc!JGs,yLi?md-qo,Lplzl,wsdTe\n",
      "QFcz,KIj-nkpMDTK qnaY-uUs\n"
     ]
    }
   ],
   "source": [
    "class NeuroBigramLM(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_emb_table = torch.nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        \"\"\"\n",
    "        idx (B,T)\n",
    "        targets (B,T)\n",
    "        \"\"\"\n",
    "\n",
    "        logits = self.token_emb_table(idx) # (B,T,C)\n",
    "        \n",
    "        if targets is not None: # compute CE_loss\n",
    "            # reshape for torch cross_entropy\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)       # (B*T, C)\n",
    "            targets = targets.view(B*T)        # (B*T)\n",
    "\n",
    "            loss = torch.nn.functional.cross_entropy(logits, targets)\n",
    "        else:\n",
    "            loss = None \n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_size):\n",
    "        for _ in range(max_new_size):\n",
    "            logits, loss = self(idx) # logits (B, T, C)\n",
    "            logits = logits[:, -1, :] # take logits only for last char (ignore others)\n",
    "            probs = torch.nn.functional.softmax(logits, dim=-1) # softmax by C\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "            \n",
    "    \n",
    "blm = NeuroBigramLM(vocab_size)\n",
    "out, loss = blm(xb, yb)\n",
    "print(f'{out.shape=}\\nCE_loss={loss}')\n",
    "\n",
    "# generate\n",
    "xval, yval = get_batch(val_data, batch_size=1)\n",
    "new = blm.generate(xval, max_new_size=100)[0].tolist()\n",
    "print(f'{decode(new)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.436866283416748\n",
      "2.428391456604004\n",
      "2.4383668899536133\n",
      "2.423478603363037\n",
      "2.440695285797119\n",
      "2.4636313915252686\n",
      "2.441060781478882\n",
      "2.517704963684082\n",
      "2.4908270835876465\n",
      "2.3524813652038574\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(blm.parameters(), lr=1e-3)\n",
    "\n",
    "batch_size = 32\n",
    "for _ in range(10000):\n",
    "    xb, yb = get_batch(train_data, batch_size)\n",
    "\n",
    "    logits, loss = blm(idx=xb, targets=yb)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if _%1000 ==0:\n",
    "        print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ir, now poual, athit gour:\n",
      "Tore; m y manththol thend ar mm, ssin sure hair m parou hom!\n",
      "I ayomer cthine SAs!\n"
     ]
    }
   ],
   "source": [
    "# generate\n",
    "xval, yval = get_batch(val_data, batch_size=1)\n",
    "new = blm.generate(xval, max_new_size=100)[0].tolist()\n",
    "print(f'{decode(new)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tril((torch.zeros(5,5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
